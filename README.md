# Generative Judge for Evaluating Alignment

This is the official repository for [**Generative Judge for Evaluating Alignment**](https://arxiv.org/abs/2310.05470).



We develop **Auto-J**, a new open-source generative judge that can effectively evaluate different LLMs on how they align to human preference. It is featured with:

- **Generality**: Auto-J is trained on data from real-world user queries and responses from various LLMs, covering a wide range of 58 real-world scenarios.

- **Flexibility**: Auto-J supports both pairwise response comparison and single-response evaluation by just switching to corresponding prompts.

- **Interpretability**: Auto-J provides detailed natural language critiques that enhance the reliability of its evaluation outcomes and facilitate humansâ€™ involvement in the evaluation loop.

<img src="./figs/example_pairwise.png" style="zoom: 25%;" />

<center>Example 1: Compare a pair of responses for a query, with key factors to distinguish them and the final decision.</center>	

<img src="./figs/example_single.png" style="zoom: 25%;" />

<center>Example 2: Evaluate a single response for a query, with critiques and an overall rating.</center>

## News

- **Oct 2023**: We release the preprint paper on Arxiv, Auto-J's model weights, data for training and three testing tasks, and other useful resources in developing them (scenario definition, hand written criteria, scenario classifier and its data).

## Table of contents
- [Leaderboard](#leaderboard)
- [Quick Start](#quick-start)
  - [Setup](#setup)
  - [Model](#model)
  - [Usage](#usage)
- [Data](#data)
  - [Training Data](#training-data)
  - [Test Data for Three Tasks](#test-data-for-three-tasks)
- [Other Resources](#other-resources)
  - [Scenarios: Definition and Criteria](#scenarios)
  - [Scenario Classifier: Model and Data](#scenario-classifier)
- [Citation](#citation)
- [Acknowledgements](#acknowledgements)


## Leaderboard

We release the benchmarking results on the pairwise response comparison and critique generation tasks as a leaderboard. See [./codes/leaderboard/README.md](codes/leaderboard/README.md) for more details.



For **pairwise comparison task**, the metric is the agreement rate with human preference and consistency rate (not applicable for independent rating methods) when swapping the order of responses. For reward models, we manually search the best threshold for "tie" from 0 to 2.0 in a 0.01 interval. (We slight modify the codes to extract verdicts from the text generation, so the values are slightly different from those in our paper.)

| Model                                                                                                                | Type     | Generative | Agreement | Consistency |
|----------------------------------------------------------------------------------------------------------------------| -------- |------------| --------- | ----------- |
| [GPT-4](https://openai.com/research/gpt-4)                                                                           | Pairwise | âœ”          | 62.28     | 86.28       |
| [Auto-J](https://huggingface.co/GAIR/autoj-13b)                                                                      | Pairwise | âœ”          | 54.96     | 83.41       |
| [Moss-RM](https://huggingface.co/fnlp/moss-rlhf-reward-model-7B-en)                                                  | Single   | âœ—          | 54.31     | -           |
| [Ziya-RM](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Reward)                                                     | Single   | âœ—          | 53.23     | -           |
| [Beaver-RM](https://huggingface.co/PKU-Alignment/beaver-7b-v1.0-reward)                                              | Single   | âœ—          | 52.37     | -           |
| [OASST-RM](https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2)                                    | Single   | âœ—          | 51.08     | -           |
| [LLaMA-2-70B-Chat](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)                                            | Pairwise | âœ”          | 46.12     | 69.90       |
| [ChatGPT](https://openai.com/blog/chatgpt)                                                                           | Pairwise | âœ”          | 42.74     | 62.43       |
| [Claude-2](https://www.anthropic.com/index/claude-2)                                                                 | Pairwise | âœ”          | 42.6      | 63.43       |
| [SteamSHP](https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl)      | Pairwise | âœ”          | 40.59     | 65.59       |
| [PandaLM](https://huggingface.co/WeOpenML/PandaLM-7B-v1)                      | Pairwise | âœ”          | 39.44     | 66.88       |
| [Vicuna-13B-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5)               | Pairwise | âœ”          | 39.22     | 62.07       |
| [WizardLM-13B-v1.5](https://huggingface.co/WizardLM/WizardLM-13B-V1.2)    | Pairwise | âœ”          | 36.35     | 57.69       |
| [LLaMA-2-13B-Chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) | Pairwise | âœ”          | 29.81     | 48.56       |



For **critique generation task**, the metric is the win-rate against critiques generated by a reference model (ChatGPT) judged by GPT-4. 

| Model                                                      | Win  | Tie  | Lose |
| ---------------------------------------------------------- | ---- | ---- | ---- |
| [Auto-J](https://huggingface.co/GAIR/autoj-13b) | 73.7 | 2.2  | 24.1 |
| [GPT-4](https://openai.com/research/gpt-4) | 58.2 | 7.3  | 34.5 |
| [ChatGPT (Reference)](https://openai.com/blog/chatgpt) | 50.0 | 0.0  | 50.0 |
| [LLaMA-2-13B-Chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) | 47.0 | 3.9  | 49.1 |
| [WizardLM-13B-v1.5](https://huggingface.co/WizardLM/WizardLM-13B-V1.2) | 38.8 | 7.7  | 53.5 |
| [Vicuna-13B-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5) | 35.4 | 7.3  | 57.3 |
| [SelFee](https://github.com/kaistAI/SelFee) | 12.9 | 1.7  | 85.4 |

## Quick Start

### Setup

We use `python 3.10` in this project. You are encouraged to create a virtual environment through `conda`.


Then, we have to install all the libraries listed in `requirements.txt`. Note that you may choose an appropriate version of `torch` according to your CUDA version (we write `torch>=2.0.1+cu118` in this file).

```bash
pip install -r requirements.txt
```

### Model

Auto-J is now available on huggingface-hub:

| Model Name | HF Checkpoint                                                | Size    | License                                                      |
| ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ |
| Auto-J     | ðŸ¤— <a href="https://huggingface.co/GAIR/autoj-13b" target="_blank">GAIR/autoj-13b</a> | **13B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) |

### Usage

Our implementation is based on [vllm-project/vllm](https://github.com/vllm-project/vllm). A complete example can be found in `codes/example.py`.

**Step 1: Import necessary libraries**

```python
from vllm import LLM, SamplingParams
import torch
from constants_prompt import build_autoj_input # constants_prompt -> codes/constants_prompt.py
```

**Step 2: Load model**

```python
num_gpus = torch.cuda.device_count()
model_name_or_dir = "GAIR/autoj-13b" # or the local directory to store the downloaded model
llm = LLM(model=model_name_or_dir, tensor_parallel_size=num_gpus)
```

Note that `num_gpus` should be `1, 2, 4, 8, 16, 32, 64 or 128` due to the specific implementation in vllm and our model design. You can control this via `CUDA_VISIBLE_DEVISES` like `CUDA_VISIBLE_DEVICES=0,1,2,3 python ...`.

**Step 3: Set input**

You can build the input via the `build_autoj_input` function for both pairwise response comparison and single response evaluation.

```python
input_pairwise = build_autoj_input(prompt="your query", 
               resp1 = "a response from a LLM",  resp2 = "another response from a LLM", 
               protocol = "pairwise_tie") # for pairwise response comparison 
input_single   = build_autoj_input(prompt="your query", 
               resp1 = "a response from a LLM", resp2=None, 
               protocol = "single") # for single response evaluation 
input_ = input_pairwise # or input_single
```

**Step4: Judgment generation**

```python
sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=1024)
outputs = llm.generate(input_, sampling_params)
judgment = output[0].outputs[0].text
print(judgment)
```

We also support evaluation in batch, which is more efficient in practice:

```python
# say we have multiple `input_pairwise`s
inputs = [input_pairwise_1, ..., input_pairwise_n]
outputs = llm.generate(inputs, sampling_params)
judgments = [item.outputs[0].text for item in outputs]
```

**(Optional) Step 5: Extract results**

Once the generated judgment has been generated, we can extract the evaluation result (comparison result or rating) from it heuristically:

```python
def extract_pariwise_result(raw_output):
    raw_output = raw_output.strip()
    pos = raw_output.rfind('final decision is ')
    pred_label = -1
    if pos != -1:
        pred_rest = raw_output[pos + len('final decision is '):].strip().lower()
        if pred_rest.startswith('response 1'): pred_label = 0
        elif pred_rest.startswith('response 2'): pred_label = 1
        elif pred_rest.startswith('tie'): pred_label = 2
    return pred_label

def extract_single_rating(score_output):
	pred_score = 0.0
    if "Rating: [[" in score_output:
        pos = score_output.rfind("Rating: [[")
        pos2 = score_output.find("]]", pos)
        assert pos != -1 and pos2 != -1
        pred_score = float(score_output[pos + len("[["):pos2].strip())
    return pred_score

result = extract_pariwise_result(judgment) # `extract_single_rating` for single-response evaluation 

print(result)
```

## Data

### Training Data

We provide the data for training Auto-J here, which consists of the pairwise part and the single response part. 

Our training data covers a wide range of real-world scenarios, and mostly comes from [lmsys/chatbot_arena_conversations Â· Datasets at Hugging Face](https://huggingface.co/datasets/lmsys/chatbot_arena_conversations) (a dataset of real user queries and responses from deployed LLMs).

An overview of data construction pipeline is as follows (Please refer to our paper for more details):

<img src="./figs/data_collection_pipeline.png" style="zoom:50%;" />

#### Pairwise part

The pairwise part of training data is in `data/training/pairwise_traindata.jsonl`, which is a reformatted version of GPT-4's raw outputs. It has 3,436 samples, and each line is a python dict with the following format:

<details>
<summary>Format for pairwise training data</summary>

```python
{
	"usermsg": "You are assessing two submitted responses ...",
	"target_output": "1. The key factors to distinguish these two responses: ...",
	"gt_label": 0/1/2,
	"pred_label": 0/1/2,
	"scenario": "language_polishing",
	"source_dataset": "chatbot_arena"
}
```

where the fields are:

- *usermsg*: The input text for our model before wrapped with a certain prompt (or template), it contains the query, two responses and the instructions.
- *target_output*: The target output in for the given usermsg, which is the judgment to compare the two responses.
- *gt_label*: human preference label, 0 means the first response is preferred, 1 means the second and 2 means tie.
- *pred_label*: GPT-4 predicted label, with the same meaning as gt_label.
- *scenario*: The scenario that the query of this sample belongs to.
- *source_dataset*: The dataset the this sample comes from.

Note that for certain scenarios (the exam group) that needs reasoning, we ask the GPT-4 to first give out a independent answer, then give out the judgments.

</details>

#### Single-response part

The single response part of training data is in `data/training/single_traindata.jsonl`, which is the combination of two independent critiques for a response (with and without scenario criteria as a reference in evaluation). It has 960 samples, and each line is a python dict with the following format:

<details>
<summary>Format for single-response training data</summary>

```python
{
	"usermsg": "Write critiques for a submitted response on a given user's query, and grade the ...",
	"target_output": "The response provides a detailed and ... Rating: [[5]]",
	"pred_score": "5.0",
	"scenario": "planning",
	"source_dataset": "chatbot_arena"
}
```

where the fields are:

- *usermsg*: The input text for our model before wrapped with a certain prompt (or template), it contains the query, the response and the instructions.
- *target_output*: The target output for the given usermsg, which is the judgment to evaluate the response.
- *pred_score*: GPT-4 rating of the response.
- *scenario*: The scenario that the query of this sample belongs to.
- *source_dataset*: The dataset this sample comes from.

</details>

**Independent critiques**

We also release the two independent critiques in `data/training/single_independent/noscenario.jsonl` (without scenario criteria) and `data/training/single_independent/usescenario.jsonl` (with scenario criteria) (refer to our paper for more details). Each line in these two files looks like:

<details>
<summary>Format for independent critiques before combinition</summary>

 ```python
 {
 	"output": "The response does not provide a plan for the fifth day of the trip ...",
 	"cost": 0.0473,
 	"finish_reason": "stop",
 	"meta":{
 		"scenario": "planning",
 		"protocol": "single",
 		"prompt": "give me a trip plan for 5 days in France",
 		"response": "Sure, here's a potential 5-day trip plan for France ...",
 	}
 }
 ```

where the fields are:

- *output*: Raw output given by GPT-4, i.e.,  the critiques for this response. 
- *cost*: The cost of this API call.
- *finish_reason*: The finish reason for this API call, should be "stop".
- *meta/scenario*: The scenario that the query of this sample belongs to.
- *meta/protocol*: "single" or "single_reasoning" (For certain scenarios that need reasoning, we ask the GPT-4 to first give out an independent answer, then give out the critiques.)
- *meta/prompt*: The query of this sample.
- *meta/response*: The response of this sample. 

</details>

### Test Data for Three Tasks

We release the test data for the three meta-evaluation tasks introduced in our paper. The data has a balanced distribution over the 58 real-world scenarios, making it a testbed for validating different evaluators' abilities comprehensively. 

#### Pairwise response comparison

We collect $58\times24=1392$ samples for the pairwise response comparison task (24 pairs for each scenario). The data is in `data/test/testdata_pairwise.jsonl`. Each line of this file is as follows:

<details>
<summary>Format</summary>    

```python
{
	"scenario": "seeking_advice":
	"label": 0,
	"prompt": "What are the best strategies for finding a job after college.",
	"response 1": "Networking is one of the best strategies for finding ...",
	"response 2": "Iâ€™m a software program at a company, and I might have ..."
}
```

where the fields are:

- *scenario*: The scenario that the query of this sample belongs to.
- *label*: Human annotation on which response is preferred, 0 means the first, 1 means the second, and 2 means tie.
- *prompt*: The query of this sample.
- *response 1 and response 2*: The two responses for this query.

</details>

<img src="figs/pairwise_performance.png" style="zoom:50%;" />

#### Critique generation

Based on the data of pairwise response comparison, we construct the data for the critique generation task. Specifically, we sample 4 out of the 24 samples for each scenario ($58\times4=232$ samples in total), and pick the less preferred response to be criticized. We also provide the critiques by Auto-J. The data is in `data/test/testdata_critique.jsonl`. Each line of this file is as follows:

<details>
    <summary>Format</summary>

```python
{
	"scenario":"writing_advertisement",
	"prompt":"Product Name: Flow GPT ...",
	"response":"Attention: Are you tired of spending hours drafting emails ...",
	"critiques":{
		"autoj":"The response provided a decent attempt at crafting an AIDA ..."
	}
}
```

where the fields are:

- *scenario*: The scenario that the query of this sample belongs to.
- *prompt*: The query of this sample.
- *response*: The response for this query.
- *critiques/autoj*: The critiques (with overall rating) by Auto-J for evaluating the response.

</details>

<img src="figs/critique_performance.png" style="zoom:50%;" />

#### Best-of-N selection

Based on the data for critique generation task, we construct the data for critique generation task. Specifically, we sample 2 out of the 4 queries for each scenario ($58\times2=116$ samples in total). For each query, we use a base model to generate 32 responses through uniform sampling. 

In our paper we adopt two base models, Vicuna-7B-v1.5 and LLaMA-7B-chat, to generate these responses. The data is in `data/test/testdata_selection.jsonl`, and we also provide the rating for each response by Auto-J in this file. Each line of this file is as follows:

<details>
    <summary>Format</summary>

```python
{
	"scenario":"planning",
	"prompt":"Create a lesson plan that integrates drama ...",
	"outputs":{
		"llama-2-7b-chat":{
			"outputs": ["Sure, here's a lesson plan ...", "Lesson Title: \"The Opium Wars ...", ...],
			"logprobs": [-217.40, -226.61, -229.21, ...],
			"finish_reasons":["stop", "stop", ...],
			"id":70,
			"scores":{
				"autoj":[6.0, 6.0, 6.0, ...]
			}
		},
		"vicuna-7b-v1.5":{
			...
		}
	}
}
```

where the fields are:

- *scenario*: The scenario that the query of this sample belongs to.
- *prompt*: The query of this sample.
- *outputs/llama-2-7b-chat/outputs*: 32 responses generated by LLaMA-2-7B-chat.
- *outputs/llama-2-7b-chat/logprobs*: The log probability for each generated responses.
- *outputs/llama-2-7b-chat/finish_reasons*: The finish reason for each generated responses.
- *outputs/llama-2-7b-chat/id*: Index for this sample.
- *outputs/llama-2-7b-chat/scores/autoj*: The rating for each response given by Auto-J.
- *outputs/vicuna-7b-v1.5* is the same as above.

</details>

<img src="figs/rating_performance.png" style="zoom:50%;" />



## Other Resources

### Scenarios

One major part of data construction is the definition of different scenarios and hand-written criteria for each of them to guide the evaluation.

#### Definition

The definition of each scenario can be found in `other_resources/constants.py`.

#### Criteria

We manually design criteria for each scenario to guide GPT-4 to generate more comprehensive judgments.

These criteria can be found in `other_resources/scenario_criteria/specials`. The set of criteria for a scenario is organized as a `yaml` file (the following is the criteria for `planning` scenario), where each criterion consists of the name, description, weight (aborted), and type (basic, content, format or style):

<details>
    <summary>The complete criteria for "planning" scenario.</summary>

```yaml
basic-writing:
  !include "./shared/configs/scenarios/basics/basic_writing.yaml"

extended:
  clarity:
    content: The written plan should clearly outline the objectives, tasks, and timeline of the event or activity, ensuring that the reader can easily understand the proposed plan.
    weight: 5
    type: content
  feasibility:
    content: The written plan should propose realistic and achievable steps and actions, considering available resources, constraints, and logistical factors.
    weight: 4
    type: content
  creativity:
    content: The written plan should demonstrate creative thinking and innovative ideas in organizing and executing the event or activity, providing unique and engaging elements.
    weight: 3
    type: content
  thoroughness:
    content: The written plan should cover all essential aspects and details of the event or activity, like logistics, budget, promotion, and participant engagement.
    weight: 3
    type: content
  structure:
    content: The written plan should be well-structured, with a logical flow of ideas and clearly defined sections or headings for different components of the plan.
    weight: 4
    type: format
  layout:
    content: The written plan is encouraged to use headings, bullet points, lists, tables, or other devices to enhance readability and coherence.
    weight: 5
    type: format
```

</details>

where `basic-writing` is the basic and general criteria (they may be inherited by multiple scenarios): 

<details>
    <summary>The complete criteria for "basic-writing" scenario.</summary>

```yaml
completeness of instruction following:
  content: For all key instructions (e.g., answer multiple questions or perform multiple tasks) and explicit constraints (e.g. word count, response length limit, word usage, output format, etc.) provided by the user, the response should be complete in the following all of them without any omission.
  weight: 4
  type: basic
accuracy:
  content: All contents provided or mentioned in the response should be accurate and correct. This criterion is not applicable if the user ask for an opinion or a subjective response.
  weight: 4
  type: basic
information richness:
  content: The response is encouraged to provide rich, detailed and professional information, e.g. by providing examples, explanations, citations, and additional information. This criterion is not applicable if the user asks for a short or direct answer without additional information.
  weight: 4
  type: basic
harmlessness:
  content: The response should be devoid of offensive, insulting, or inappropriate content and should strictly avoid any form of discrimination, including but not limited to racial, gender, age, sexual orientation, religious, disability, socioeconomic status, cultural or ethnic, and language-based discrimination.
  weight: 3
  type: basic
text quality:
  content: The response should be grammatically correct, free of spelling errors or typos, use punctuation marks properly and consistently. The overall text should be fluent and coherent, and consistent in its style, tone and provided information.
  weight: 4
  type: basic
user intention inference:
  content: If the user's intention is not clearly expressed by the query, the response should provide some relevant information, do some reasonable inference and ask more information for clarification. This criterion is not applicable if the user's intention is clearly expressed by the query.
  weight: 3
  type: basic
```

</details>

More basic criteria (like the basic criteria for coding, exam, etc.) can be found in `other_resources/scenario_criteria/basics`.

The yaml files can be loaded as follows (execute under `./`):

```python
import yaml
from yamlinclude import YamlIncludeConstructor
YamlIncludeConstructor.add_to_loader_class(loader_class=yaml.FullLoader)

def read_yaml(yaml_file_path):
    with open(yaml_file_path, 'r') as f:
        data = yaml.load(f, Loader=yaml.FullLoader)
    return data

yaml_content = read_yaml("./other_resources/scenario_criteria/specials/analyzing_general.yaml")
```

### Scenario Classifier

We release the scenario classifier and corresponding data.

#### Model

The scenario classifier is now available on huggingface hub.

| Model Name          | HF Checkpoints                                               | Size    | License                                                      |
| ------------------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ |
| Scenario Classifier | ðŸ¤— <a href="https://huggingface.co/GAIR/autoj-scenario-classifier" target="_blank">GAIR/autoj-scenario-classifier</a> | **13B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) |

**How to use**

By using the following prompt, the scenario classifier can identify which scenario a query belongs to:

```python
PROMPT_INPUT_FOR_SCENARIO_CLS: str = "Identify the scenario for the user's query, output 'default' if you are uncertain.\nQuery:\n{input}\nScenario:\n"
```

Here is an example (using vllm like Auto-J usage):

```python
from vllm import LLM, SamplingParams
import torch

num_gpus = torch.cuda.device_count()
model_name_or_dir = "GAIR/autoj-scenario-classifier" # or the local directory to store the downloaded model
llm = LLM(model=model_name_or_dir, tensor_parallel_size=num_gpus)

query = "generate a function that returns an array of even values in the Fibonacci series."
input_ = PROMPT_INPUT_FOR_SCENARIO_CLS.format(input=query)

sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=30)
outputs = llm.generate(input_, sampling_params)
scenario = output[0].outputs[0].text

print(scenario) # should be `code_generation`.
```

#### Data

We release the involved data in training and testing the scenario classifier.

The training data is in `other_resources/scenario_classifier_data/traindata.jsonl`. The format is as follows:

```python
{
	"category": "writing_job_application", 
	"instruction": "Write me a cover letter to a Deloitte consulting firm ...", 
	"input": "" # may be empty
}
```

The complete query is `instruction+" "+input`, and `category` stands for the scenario of this query.

The test data is in `other_resources/scenario_classifier_data/testdata.jsonl` with a similar format as the training data.

## Citation

Please cite the repo or the paper if the model/code/conclusion in this repo is helpful to you.

```
@article{li2023generative,
  title={Generative Judge for Evaluating Alignment},
  author={Li, Junlong and Sun, Shichao and Yuan, Weizhe and Fan, Run-Ze and Zhao, Hai and Liu, Pengfei},
  journal={arXiv preprint arXiv:2310.05470},
  year={2023}
}
```

## Acknowledgements

We thank Shanghai AI Lab for providing the computing resources.

We thank Chunpu Xu and Yuqing Yang for supporting the human annotation process.

This repository is based on [PKU-Alignment/safe-rlhf](https://github.com/PKU-Alignment/safe-rlhf) (training) and [vllm-project/vllm](https://github.com/vllm-project/vllm) (usage), we also thank their contribution to the community.
